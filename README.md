# **Ex.No.2 :Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: ChatGPT, Claude, Bard, Cohere Command, and Meta**

### **REGISTER NUMBER: 212222210021**

### **NAME: Rishanth S**

---

## **Aim:**

To compare the performance, user experience, and response quality of different AI platforms—ChatGPT, Claude, Bard, Cohere Command, and Meta—using a common prompt-based use case. The objective is to generate outputs using these 2024 prompting tools and evaluate them based on accuracy, clarity, depth, and relevance.

---

## **AI Tools Required:**

* **ChatGPT (OpenAI)**
* **Claude (Anthropic)**
* **Bard / Gemini (Google)**
* **Cohere Command**
* **Meta Llama Model**

---

# **Explanation**

### **1. Define the Use Case:**

The selected use case for evaluation:
**“Summarize a technical paragraph about Artificial Intelligence.”**
This task is compatible with all AI platforms and allows direct comparison of summarization accuracy, clarity, and depth.

---

### **2. Create a Uniform Set of Prompts:**

The following prompt was used across all 5 tools:

**Prompt:**
“Summarize the following text in 4–5 lines:
‘Artificial Intelligence (AI) refers to the simulation of human intelligence in machines designed to think and act like humans. Modern AI systems can learn from large datasets, adapt to different tasks, and improve through experience. AI has applications in healthcare, robotics, finance, and autonomous systems, significantly transforming industries worldwide.’”

---

### **3. Run the Experiment on Each AI Platform:**

The above prompt was given to:

* ChatGPT
* Claude
* Bard/Gemini
* Cohere Command
* Meta Llama

Their responses were recorded under the same conditions:

* Same input prompt
* Same format
* No additional context
* Standard mode for each tool

Response time, clarity, detail, and ease of use were noted.

---

## **Outputs Collected From Each Platform**

### **1. ChatGPT Output:**

A well-structured and concise summary, maintaining all major points while improving readability.

### **2. Claude Output:**

Clear, human-like explanation with emphasis on reasoning and flow.

### **3. Bard/Gemini Output:**

Factually strong summary, slightly more academic in tone.

### **4. Cohere Command Output:**

Shorter summary with less depth, more direct and simplified.

### **5. Meta Llama Output:**

Clear but minimal; maintains factual accuracy, but with limited detail.

---

# **Comparison Table**

| **AI Tool**        | **Accuracy** | **Clarity** | **Depth** | **Relevance** |
| ------------------ | ------------ | ----------- | --------- | ------------- |
| **ChatGPT**        | ★★★★★        | ★★★★★       | ★★★★★     | ★★★★★         |
| **Claude**         | ★★★★★        | ★★★★★       | ★★★★☆     | ★★★★★         |
| **Bard/Gemini**    | ★★★★☆        | ★★★★★       | ★★★★☆     | ★★★★★         |
| **Cohere Command** | ★★★☆☆        | ★★★★☆       | ★★★☆☆     | ★★★★☆         |
| **Meta Llama**     | ★★★★☆        | ★★★☆☆       | ★★★☆☆     | ★★★☆☆         |

---

# **Final Report / Findings**

Based on the experiment, the following observations were made:

### **ChatGPT:**

* Provides the strongest balance of accuracy, depth, and clarity.
* Best for academic and professional summarization tasks.

### **Claude:**

* Very human-like responses with excellent clarity.
* Excels at reasoning, slightly longer summaries.

### **Bard / Gemini:**

* Strong factual accuracy, performs well in technical content.
* Best suited for research-focused users.

### **Cohere Command:**

* Good for short, quick summaries.
* Limited depth but fast and efficient.

### **Meta Llama:**

* Produces simple and clear outputs.
* Best for basic summarization; lacks advanced depth.

### **Overall Recommendation:**

* **Best for Technical Summaries:** ChatGPT / Claude
* **Best for Academic Tone:** Bard/Gemini
* **Best for Fast Lightweight Summaries:** Cohere
* **Best Open-Model Simplicity:** Meta Llama

---

# **Output:**

The outputs from all five AI tools were successfully collected, evaluated, and compared based on the defined criteria: accuracy, clarity, depth, and relevance.

---

# **Conclusion:**

The experiment successfully demonstrated how different 2024 AI prompting tools respond to the same task. Each platform showed distinct strengths—ChatGPT excelled in depth and clarity, Claude in reasoning quality, Bard in factual consistency, Cohere in simplicity, and Meta in general usability. These results highlight that the choice of AI tool should depend on the specific use case, output requirements, and desired level of detail.

---

# **Result:**

The prompt for the above problem statement was executed successfully across all AI platforms.

---
